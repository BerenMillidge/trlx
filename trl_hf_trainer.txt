Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.1.attn.masked_bias', 'h.10.attn.masked_bias', 'h.2.attn.masked_bias', 'h.7.attn.masked_bias', 'h.4.attn.masked_bias', 'h.9.attn.masked_bias', 'h.8.attn.masked_bias', 'lm_head.weight', 'h.11.attn.masked_bias', 'h.6.attn.masked_bias', 'h.3.attn.masked_bias', 'h.0.attn.masked_bias', 'v_head.summary.bias', 'v_head.summary.weight', 'h.5.attn.masked_bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of GPT2HeadWithValueModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.1.attn.masked_bias', 'h.10.attn.masked_bias', 'h.2.attn.masked_bias', 'h.7.attn.masked_bias', 'h.4.attn.masked_bias', 'h.9.attn.masked_bias', 'h.8.attn.masked_bias', 'lm_head.weight', 'h.11.attn.masked_bias', 'h.6.attn.masked_bias', 'h.3.attn.masked_bias', 'h.0.attn.masked_bias', 'v_head.summary.bias', 'v_head.summary.weight', 'h.5.attn.masked_bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Reusing dataset imdb (/home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)
Loading cached processed dataset at /home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-a953b14d316a74f4.arrow
Loading cached processed dataset at /home/alex/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-80b2ed26b0490c19.arrow
max_steps is given, it will override any value given in num_train_epochs
/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 23650
  Num Epochs = 1
  Instantaneous batch size per device = 16
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 1
  Total optimization steps = 1000
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: dahoas. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.3 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.2
wandb: Run data is saved locally in /fsx/alex/repos/trl/wandb/run-20220915_202146-jz5dj1fz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ./results
wandb: ‚≠êÔ∏è View project at https://wandb.ai/dahoas/huggingface
wandb: üöÄ View run at https://wandb.ai/dahoas/huggingface/runs/jz5dj1fz
  0%|          | 0/1000 [00:00<?, ?it/s]The following columns in the training set don't have a corresponding argument in `GPT2HeadWithValueModel.forward` and have been ignored: text. If text are not expected by `GPT2HeadWithValueModel.forward`,  you can safely ignore this message.
/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/text_classification.py:89: UserWarning: `return_all_scores` is now deprecated, use `top_k=1` if you want similar functionnality
  warnings.warn(
Disabling tokenizer parallelism, we're using DataLoader multithreading already
  0%|          | 1/1000 [00:02<38:05,  2.29s/it]  0%|          | 2/1000 [00:03<28:02,  1.69s/it]  0%|          | 3/1000 [00:04<24:50,  1.49s/it]  0%|          | 4/1000 [00:06<23:22,  1.41s/it]  0%|          | 5/1000 [00:07<22:29,  1.36s/it]  1%|          | 6/1000 [00:08<21:56,  1.32s/it]  1%|          | 7/1000 [00:09<21:36,  1.31s/it]  1%|          | 8/1000 [00:11<22:29,  1.36s/it]  1%|          | 9/1000 [00:12<21:56,  1.33s/it]  1%|          | 10/1000 [00:13<21:32,  1.31s/it]                                                   1%|          | 10/1000 [00:13<21:32,  1.31s/it]/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/base.py:1036: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset
  warnings.warn(
  1%|          | 11/1000 [00:15<21:24,  1.30s/it]  1%|          | 12/1000 [00:16<21:12,  1.29s/it]  1%|‚ñè         | 13/1000 [00:17<21:04,  1.28s/it]  1%|‚ñè         | 14/1000 [00:18<20:55,  1.27s/it]  2%|‚ñè         | 15/1000 [00:20<20:48,  1.27s/it]  2%|‚ñè         | 16/1000 [00:21<20:47,  1.27s/it]  2%|‚ñè         | 17/1000 [00:22<20:39,  1.26s/it]  2%|‚ñè         | 18/1000 [00:24<21:18,  1.30s/it]  2%|‚ñè         | 19/1000 [00:25<21:02,  1.29s/it]  2%|‚ñè         | 20/1000 [00:26<20:50,  1.28s/it]                                                   2%|‚ñè         | 20/1000 [00:26<20:50,  1.28s/it]  2%|‚ñè         | 21/1000 [00:27<20:42,  1.27s/it]  2%|‚ñè         | 22/1000 [00:29<20:34,  1.26s/it]  2%|‚ñè         | 23/1000 [00:30<21:09,  1.30s/it]  2%|‚ñè         | 24/1000 [00:31<20:54,  1.29s/it]  2%|‚ñé         | 25/1000 [00:33<20:45,  1.28s/it]  3%|‚ñé         | 26/1000 [00:34<20:36,  1.27s/it]  3%|‚ñé         | 27/1000 [00:35<20:29,  1.26s/it]  3%|‚ñé         | 28/1000 [00:36<20:25,  1.26s/it]  3%|‚ñé         | 29/1000 [00:38<20:21,  1.26s/it]  3%|‚ñé         | 30/1000 [00:39<21:00,  1.30s/it]                                                   3%|‚ñé         | 30/1000 [00:39<21:00,  1.30s/it]  3%|‚ñé         | 31/1000 [00:40<20:47,  1.29s/it]  3%|‚ñé         | 32/1000 [00:41<20:36,  1.28s/it]  3%|‚ñé         | 33/1000 [00:43<20:28,  1.27s/it]  3%|‚ñé         | 34/1000 [00:44<20:23,  1.27s/it]  4%|‚ñé         | 35/1000 [00:45<20:59,  1.30s/it]  4%|‚ñé         | 36/1000 [00:47<20:42,  1.29s/it]  4%|‚ñé         | 37/1000 [00:48<20:30,  1.28s/it]  4%|‚ñç         | 38/1000 [00:49<20:21,  1.27s/it]  4%|‚ñç         | 39/1000 [00:50<20:13,  1.26s/it]  4%|‚ñç         | 40/1000 [00:52<20:53,  1.31s/it]                                                   4%|‚ñç         | 40/1000 [00:52<20:53,  1.31s/it]  4%|‚ñç         | 41/1000 [00:53<20:36,  1.29s/it]  4%|‚ñç         | 42/1000 [00:54<20:21,  1.28s/it]  4%|‚ñç         | 43/1000 [00:55<20:15,  1.27s/it]  4%|‚ñç         | 44/1000 [00:57<20:09,  1.27s/it]  4%|‚ñç         | 45/1000 [00:58<20:03,  1.26s/it]  5%|‚ñç         | 46/1000 [00:59<19:57,  1.26s/it]  5%|‚ñç         | 47/1000 [01:00<19:52,  1.25s/it]  5%|‚ñç         | 48/1000 [01:02<19:49,  1.25s/it]  5%|‚ñç         | 49/1000 [01:03<19:46,  1.25s/it]  5%|‚ñå         | 50/1000 [01:04<19:43,  1.25s/it]                                                   5%|‚ñå         | 50/1000 [01:04<19:43,  1.25s/it]  5%|‚ñå         | 51/1000 [01:05<19:41,  1.25s/it]  5%|‚ñå         | 52/1000 [01:07<19:41,  1.25s/it]  5%|‚ñå         | 53/1000 [01:08<19:42,  1.25s/it]  5%|‚ñå         | 54/1000 [01:09<19:39,  1.25s/it]  6%|‚ñå         | 55/1000 [01:10<19:37,  1.25s/it]  6%|‚ñå         | 56/1000 [01:12<20:03,  1.27s/it]  6%|‚ñå         | 57/1000 [01:14<24:23,  1.55s/it]  6%|‚ñå         | 58/1000 [01:15<22:56,  1.46s/it]  6%|‚ñå         | 59/1000 [01:16<21:55,  1.40s/it]  6%|‚ñå         | 60/1000 [01:18<21:12,  1.35s/it]                                                   6%|‚ñå         | 60/1000 [01:18<21:12,  1.35s/it]  6%|‚ñå         | 61/1000 [01:19<20:40,  1.32s/it]  6%|‚ñå         | 62/1000 [01:20<20:57,  1.34s/it]  6%|‚ñã         | 63/1000 [01:22<20:31,  1.31s/it]  6%|‚ñã         | 64/1000 [01:23<20:12,  1.29s/it]  6%|‚ñã         | 65/1000 [01:24<19:58,  1.28s/it]  7%|‚ñã         | 66/1000 [01:25<19:48,  1.27s/it]  7%|‚ñã         | 67/1000 [01:27<20:27,  1.32s/it]  7%|‚ñã         | 68/1000 [01:28<20:48,  1.34s/it]  7%|‚ñã         | 69/1000 [01:30<21:05,  1.36s/it]  7%|‚ñã         | 70/1000 [01:31<21:51,  1.41s/it]                                                   7%|‚ñã         | 70/1000 [01:31<21:51,  1.41s/it]  7%|‚ñã         | 71/1000 [01:32<21:06,  1.36s/it]  7%|‚ñã         | 72/1000 [01:34<20:32,  1.33s/it]  7%|‚ñã         | 73/1000 [01:35<20:09,  1.30s/it]  7%|‚ñã         | 74/1000 [01:36<19:52,  1.29s/it]  8%|‚ñä         | 75/1000 [01:37<19:39,  1.27s/it]  8%|‚ñä         | 76/1000 [01:39<19:30,  1.27s/it]  8%|‚ñä         | 77/1000 [01:40<19:24,  1.26s/it]  8%|‚ñä         | 78/1000 [01:41<19:19,  1.26s/it]  8%|‚ñä         | 79/1000 [01:42<19:15,  1.25s/it]  8%|‚ñä         | 80/1000 [01:44<19:12,  1.25s/it]                                                   8%|‚ñä         | 80/1000 [01:44<19:12,  1.25s/it]  8%|‚ñä         | 81/1000 [01:45<19:10,  1.25s/it]  8%|‚ñä         | 82/1000 [01:46<19:56,  1.30s/it]  8%|‚ñä         | 83/1000 [01:48<19:37,  1.28s/it]  8%|‚ñä         | 84/1000 [01:49<19:24,  1.27s/it]  8%|‚ñä         | 85/1000 [01:50<19:13,  1.26s/it]  9%|‚ñä         | 86/1000 [01:51<19:07,  1.26s/it]  9%|‚ñä         | 87/1000 [01:52<19:04,  1.25s/it]  9%|‚ñâ         | 88/1000 [01:54<19:00,  1.25s/it]  9%|‚ñâ         | 89/1000 [01:55<18:56,  1.25s/it]  9%|‚ñâ         | 90/1000 [01:56<18:52,  1.24s/it]                                                   9%|‚ñâ         | 90/1000 [01:56<18:52,  1.24s/it]  9%|‚ñâ         | 91/1000 [01:57<18:51,  1.25s/it]  9%|‚ñâ         | 92/1000 [01:59<18:49,  1.24s/it]  9%|‚ñâ         | 93/1000 [02:00<18:47,  1.24s/it]  9%|‚ñâ         | 94/1000 [02:01<19:24,  1.29s/it] 10%|‚ñâ         | 95/1000 [02:03<19:11,  1.27s/it] 10%|‚ñâ         | 96/1000 [02:04<19:01,  1.26s/it] 10%|‚ñâ         | 97/1000 [02:05<18:53,  1.26s/it] 10%|‚ñâ         | 98/1000 [02:06<18:48,  1.25s/it] 10%|‚ñâ         | 99/1000 [02:08<19:15,  1.28s/it] 10%|‚ñà         | 100/1000 [02:09<19:31,  1.30s/it]                                                   10%|‚ñà         | 100/1000 [02:09<19:31,  1.30s/it] 10%|‚ñà         | 101/1000 [02:10<19:17,  1.29s/it] 10%|‚ñà         | 102/1000 [02:12<20:03,  1.34s/it] 10%|‚ñà         | 103/1000 [02:13<19:37,  1.31s/it] 10%|‚ñà         | 104/1000 [02:14<20:02,  1.34s/it] 10%|‚ñà         | 105/1000 [02:16<19:37,  1.32s/it] 11%|‚ñà         | 106/1000 [02:17<19:19,  1.30s/it] 11%|‚ñà         | 107/1000 [02:18<19:04,  1.28s/it] 11%|‚ñà         | 108/1000 [02:19<18:53,  1.27s/it] 11%|‚ñà         | 109/1000 [02:21<19:21,  1.30s/it] 11%|‚ñà         | 110/1000 [02:22<19:05,  1.29s/it]                                                   11%|‚ñà         | 110/1000 [02:22<19:05,  1.29s/it] 11%|‚ñà         | 111/1000 [02:23<18:54,  1.28s/it] 11%|‚ñà         | 112/1000 [02:24<18:45,  1.27s/it] 11%|‚ñà‚ñè        | 113/1000 [02:26<18:40,  1.26s/it] 11%|‚ñà‚ñè        | 114/1000 [02:27<18:43,  1.27s/it] 12%|‚ñà‚ñè        | 115/1000 [02:28<18:50,  1.28s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors
{'loss': 2.5281, 'learning_rate': 5e-06, 'epoch': 0.01}
{'loss': 3.5477, 'learning_rate': 1e-05, 'epoch': 0.01}
{'loss': 70369.8062, 'learning_rate': 1.5e-05, 'epoch': 0.02}
{'loss': 67.13, 'learning_rate': 2e-05, 'epoch': 0.03}
{'loss': 1124591616.0, 'learning_rate': 2.5e-05, 'epoch': 0.03}
{'loss': 6.5731, 'learning_rate': 3e-05, 'epoch': 0.04}
{'loss': 2.4902, 'learning_rate': 3.5e-05, 'epoch': 0.05}
{'loss': 3.9745, 'learning_rate': 4e-05, 'epoch': 0.05}
{'loss': 1.5981, 'learning_rate': 4.5e-05, 'epoch': 0.06}
{'loss': 1.2023, 'learning_rate': 5e-05, 'epoch': 0.07}
{'loss': 1.8144, 'learning_rate': 4.9444444444444446e-05, 'epoch': 0.07}
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.325 MB uploaded (0.000 MB deduped)wandb: \ 0.319 MB of 0.325 MB uploaded (0.000 MB deduped)wandb: | 0.325 MB of 0.325 MB uploaded (0.000 MB deduped)wandb: / 0.325 MB of 0.325 MB uploaded (0.000 MB deduped)wandb: - 0.325 MB of 0.325 MB uploaded (0.000 MB deduped)wandb: \ 0.325 MB of 0.325 MB uploaded (0.000 MB deduped)wandb: | 0.325 MB of 0.325 MB uploaded (0.000 MB deduped)wandb: / 0.325 MB of 0.325 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:        mean_rewards ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÅ
wandb:         train/epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà
wandb:   train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: train/learning_rate ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà
wandb:          train/loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:        mean_rewards -1.31614
wandb:         train/epoch 0.07
wandb:   train/global_step 110
wandb: train/learning_rate 5e-05
wandb:          train/loss 1.8144
wandb: 
wandb: Synced ./results: https://wandb.ai/dahoas/huggingface/runs/jz5dj1fz
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20220915_202146-jz5dj1fz/logs
Traceback (most recent call last):
  File "test_trl_hf_trainer.py", line 116, in <module>
    trainer.train()
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/trainer.py", line 1498, in train
    return inner_training_loop(
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/trainer.py", line 1740, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/trainer.py", line 2470, in training_step
    loss = self.compute_loss(model, inputs)
  File "/fsx/alex/repos/trl/ppo_hf_trainer.py", line 149, in compute_loss
    rewards = torch.tensor([output[1]["score"] for output in pipe_outputs]).to(self.model.device)  # TODO(dahoas): Correct device?
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/text_classification.py", line 138, in __call__
    result = super().__call__(*args, **kwargs)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/base.py", line 1056, in __call__
    outputs = [output for output in final_iterator]
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/base.py", line 1056, in <listcomp>
    outputs = [output for output in final_iterator]
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/pt_utils.py", line 111, in __next__
    item = next(self.iterator)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/pt_utils.py", line 112, in __next__
    processed = self.infer(item, **self.params)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/base.py", line 983, in forward
    model_outputs = self._forward(model_inputs, **forward_params)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/pipelines/text_classification.py", line 165, in _forward
    return self.model(**model_inputs)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 749, in forward
    distilbert_output = self.distilbert(
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 568, in forward
    inputs_embeds = self.embeddings(input_ids)  # (bs, seq_length, dim)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/fsx/alex/.envs/accelerate/lib64/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py", line 132, in forward
    embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)
RuntimeError: The size of tensor a (1210) must match the size of tensor b (512) at non-singleton dimension 1
